###Machine Learning Work

First, we will load all the required packages for work and read the training csv file. For convenience we will convert it to tbl_df format.
```{r message=FALSE, cache=TRUE}
library(dplyr)
library(ggplot2)
library(caret)
setwd("C:\\Users\\barca\\Desktop\\MachineLearningExam")
train_full <- read.csv("pml-training.csv", header = TRUE, sep = ",", na.strings = c("NA", ""), stringsAsFactors = TRUE)
train_full <- tbl_df(train_full)
```

After studying the data it becomes clear, that many variables have *more than 50%* missing values rate (mostly even more than 90%), so it is more reasonable to exclude this variables, then trying to substitute to some logical values, is missing values are actually more than normal.
```{r cache = TRUE}
cols <- ncol(train_full)
rows <- nrow(train_full)
knockout <- character(0)
for(i in 1:cols) {
       if(sum(is.na(train_full[,i])) < rows*0.50)
          {knockout <- c(knockout, names(train_full[i]))} }
```

We will also exclude *time series variables*, because we are not going to use time-series forecasting, as well as we will include "X" variable, because it is just ordering from 1 to N.
```{r cache = TRUE}
train_2 <- train_full %>% select(knockout)
train_3 <- train_2 %>% select(-c(X, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp))
```


For validation we will use the following two techinques.

- We will split the training set into 75% training_final and 25% testing_final set, and leave the real testing set with 20 variables as validation test.
- We will use cross-validation on train_final with 3 folds, as a first step to estimate out-of-sample error, before trying the model on testing. We will only proceed to train_final, if it shows accuracy rate > 95%. 

```{r cache = TRUE}
set.seed(7)
splits <- createDataPartition(train_3$classe, p = 0.75, list = F)
train_final <- train_3[splits,]
test_final <-  train_3[-splits,]
train_options <- trainControl(method = "cv", number = 3, verboseIter = FALSE) ## Cross-validation
```


As this model is going to work as a black-box prediction (we have more than 50 variables predictor), we will first try to use random forest algorithm.
```{r cache = TRUE}
model1 <- train(classe ~ ., data = train_final, method = "rf", trControl = train_options)
model1$results
confusionMatrix(predict(model1), train_final$classe)
```

As the cross-validation results show, predicted Out-of-Sample Accuracy is more than 99%. So we can proceed and predict with this model on test_final dataset.


```{r cache = TRUE}
confusionMatrix(predict(model1, newdata = test_final), test_final$classe)
```
The resulting accuracy is great, thus we don't need algorithms and can proceed to validation set.


```{r cache = TRUE}
validation20 <- train_full <- read.csv("pml-testing.csv", header = TRUE, sep = ",", na.strings = c("NA", ""), stringsAsFactors = TRUE)
predict(model1, newdata = validation20)
```
All the 20 predicted values were correct according to the test quiz.